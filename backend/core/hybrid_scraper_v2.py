"""
üöÄ Hybrid Scraper V2 - Browser Pool Integration
Sistema ultra-stabile con pool di browser e timeout adattivi
"""

import asyncio
import time
import logging
import os
from typing import Dict, Any, Optional
from dataclasses import dataclass

from .browser_pool import browser_pool
from .advanced_scraper import advanced_scraper
from .keyword_extraction import extract_keywords_from_content
from .ua_rotator import ua_rotator
from .proxy_system import proxy_system
from .domain_intelligence import should_skip_scraping, get_domain_config

logger = logging.getLogger(__name__)

@dataclass
class ScrapingResult:
    """Risultato operazione scraping"""
    success: bool
    content: str = ""
    error: str = ""
    method: str = ""
    duration: float = 0.0
    content_length: int = 0

class HybridScraperV2:
    """
    üõ°Ô∏è Sistema di scraping con resilienza estrema:
    1. Browser Pool (Primario) - Ultra stabile
    2. ScrapingBee (Secondario) - Cloud service 
    3. Advanced Scraper (Terziario) - Fallback
    4. Basic HTTP (Quaternario) - Last resort
    """
    
    def __init__(self):
        self.scrapingbee_api_key = os.getenv('SCRAPINGBEE_API_KEY')
        self.scraping_mode = os.getenv('SCRAPING_MODE', 'development')
        
        # üìä Statistiche performance dettagliate
        self.stats = {
            'browser_pool_success': 0,
            'scrapingbee_success': 0,
            'advanced_success': 0,
            'basic_success': 0,
            'total_requests': 0,
            'total_failures': 0,
            'average_duration': 0.0,
            'success_rate': 0.0,
            'method_distribution': {},
            'error_types': {}
        }
    
    async def scrape_intelligent(self, url: str, max_keywords: int = 20, use_advanced: bool = True) -> Dict[str, Any]:
        """
        üß† Scraping super-intelligente con DOPPIO FALLBACK UNIFICATO
        
        ‚ö° STESSO METODO PER CLIENT E COMPETITOR:
        1. Basic HTTP (veloce, 5s timeout)
        2. Browser Pool fallback (se Basic fallisce o content < 1000 chars)
        
        Questo garantisce success rate 95%+ come nell'analisi client.
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        logger.info(f"üéØ Starting UNIFIED scrape with INTELLIGENT FALLBACK for: {url}")
        
        # üèÜ LAYER 1: Basic HTTP (veloce, prima scelta)
        logger.info(f"üöÄ Layer 1: Trying Basic HTTP first...")
        result = await self._scrape_basic(url)
        logger.info(f"üîç Basic HTTP result: success={result.success}, content_length={result.content_length if result.success else 0}, error={result.error if not result.success else 'None'}")
        
        # ‚úÖ Validazione qualit√† contenuto (come in ai_site_analyzer.py)
        content_sufficient = result.success and result.content_length >= 1000
        
        if content_sufficient:
            keywords_data = await self._extract_keywords_smart(result.content, url, max_keywords)
            self._update_stats('basic_success', result.method, result.duration)
            logger.info(f"‚úÖ Basic HTTP SUCCESS: {len(keywords_data.get('keywords', []))} keywords")
            return keywords_data
        
        # üîÑ LAYER 2: Browser Pool fallback (se Basic fallisce o content insufficiente)
        if not result.success:
            logger.warning(f"‚ö†Ô∏è Basic HTTP FAILED: {result.error}")
        else:
            logger.warning(f"‚ö†Ô∏è Content insufficient ({result.content_length} < 1000 chars)")
        
        logger.info(f"üîÑ Layer 2: Trying Browser Pool fallback...")
        
        # Check se Browser Pool √® disponibile (Railway protection)
        if not browser_pool.is_initialized:
            logger.error(f"‚ùå Browser Pool not initialized - cannot fallback")
            # Usa il risultato Basic HTTP anche se insufficiente
            if result.success and result.content:
                logger.warning(f"‚ö†Ô∏è Using Basic HTTP result anyway ({result.content_length} chars)")
                keywords_data = await self._extract_keywords_smart(result.content, url, max_keywords)
                self._update_stats('basic_success', result.method, result.duration)
                return keywords_data
        
        # Prova Browser Pool fallback
        browser_result = await self._scrape_with_browser_pool(url)
        logger.info(f"üîç Browser Pool result: success={browser_result.success}, error={browser_result.error if not browser_result.success else 'None'}")
        
        if browser_result.success:
            keywords_data = await self._extract_keywords_smart(browser_result.content, url, max_keywords)
            self._update_stats('browser_pool_success', browser_result.method, browser_result.duration)
            logger.info(f"‚úÖ Browser Pool SUCCESS: {len(keywords_data.get('keywords', []))} keywords")
            return keywords_data
        else:
            logger.error(f"‚ùå Browser Pool FAILED: {browser_result.error}")
        
        # ‚ùå Entrambi i metodi falliti
        total_duration = time.time() - start_time
        self.stats['total_failures'] += 1
        self._update_error_stats(browser_result.error)
        
        logger.error(f"‚ùå ALL METHODS FAILED for {url} after {total_duration:.2f}s")
        logger.error(f"   - Basic HTTP: {result.error}")
        logger.error(f"   - Browser Pool: {browser_result.error}")
        
        return {
            'url': url,
            'keywords': [],
            'status': 'failed',
            'error': f"All scraping methods failed. Basic HTTP: {result.error}, Browser Pool: {browser_result.error}",
            'scraping_method': 'none',
            'duration': total_duration
        }
    
    async def _scrape_with_browser_pool(self, url: str) -> ScrapingResult:
        """üèä‚Äç‚ôÇÔ∏è Scraping con Browser Pool - MASSIMA STABILIT√Ä con TIMEOUT INTELLIGENTE"""
        start_time = time.time()
        
        try:
            # üö® Check if Browser Pool is available (Railway resource protection)
            if not browser_pool.is_initialized:
                logger.warning("‚ö†Ô∏è Browser Pool not initialized - skipping (Railway RAM protection)")
                logger.info(f"‚ÑπÔ∏è  Browser Pool status: initialized={browser_pool.is_initialized}, pool_size={len(browser_pool.session_pool) if hasattr(browser_pool, 'session_pool') else 0}")
                return ScrapingResult(
                    success=False,
                    error="Browser pool not initialized (resource protection)",
                    method="browser_pool",
                    duration=time.time() - start_time
                )
            
            logger.info(f"‚úÖ Browser Pool available - attempting scrape for {url}")
            
            # Ottieni sessione dal pool
            session = await browser_pool.get_session()
            
            if not session:
                return ScrapingResult(
                    success=False,
                    error="Browser pool not available",
                    method="browser_pool",
                    duration=time.time() - start_time
                )
            
            # Scraping con sessione pooled con TIMEOUT INTELLIGENTE (15s max)
            content = await asyncio.wait_for(
                browser_pool.scrape_with_session(session, url),
                timeout=15.0  # 15s invece di 30s default
            )
            duration = time.time() - start_time
            
            if content and len(content) > 500:
                return ScrapingResult(
                    success=True,
                    content=content,
                    method="browser_pool",
                    duration=duration,
                    content_length=len(content)
                )
            else:
                return ScrapingResult(
                    success=False,
                    error=f"Insufficient content: {len(content)} chars",
                    method="browser_pool",
                    duration=duration
                )
                
        except asyncio.TimeoutError:
            return ScrapingResult(
                success=False,
                error="Browser pool timeout (15s)",
                method="browser_pool", 
                duration=time.time() - start_time
            )
        except Exception as e:
            return ScrapingResult(
                success=False,
                error=str(e),
                method="browser_pool",
                duration=time.time() - start_time
            )
    
    async def _scrape_with_scrapingbee(self, url: str) -> ScrapingResult:
        """üêù Scraping con ScrapingBee API"""
        import aiohttp
        
        start_time = time.time()
        
        try:
            params = {
                'api_key': self.scrapingbee_api_key,
                'url': url,
                'render_js': 'true',
                'premium_proxy': 'true',
                'country_code': 'IT',
                'wait': '3000',
                'window_width': '1366',
                'window_height': '768',
                'block_ads': 'true'
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                async with session.get('https://app.scrapingbee.com/api/v1/', params=params) as response:
                    duration = time.time() - start_time
                    
                    # ‚úÖ Accept all 2xx status codes for ScrapingBee too
                    if 200 <= response.status < 300:
                        content = await response.text()
                        return ScrapingResult(
                            success=True,
                            content=content,
                            method="scrapingbee",
                            duration=duration,
                            content_length=len(content)
                        )
                    else:
                        error_text = await response.text()
                        return ScrapingResult(
                            success=False,
                            error=f"ScrapingBee HTTP {response.status}: {error_text}",
                            method="scrapingbee",
                            duration=duration
                        )
        
        except Exception as e:
            return ScrapingResult(
                success=False,
                error=str(e),
                method="scrapingbee",
                duration=time.time() - start_time
            )
    
    async def _scrape_with_advanced(self, url: str) -> ScrapingResult:
        """ü•∑ Advanced Scraper con stealth + TIMEOUT INTELLIGENTE"""
        start_time = time.time()
        
        try:
            # Advanced scraper con timeout intelligente (20s max)
            content = await asyncio.wait_for(
                advanced_scraper.intelligent_scrape(url, max_retries=1),  # Reduced retries
                timeout=20.0  # 20s invece di 30s default
            )
            duration = time.time() - start_time
            
            if content and len(content) > 500:
                return ScrapingResult(
                    success=True,
                    content=content,
                    method="advanced",
                    duration=duration,
                    content_length=len(content)
                )
            else:
                return ScrapingResult(
                    success=False,
                    error=f"Insufficient content: {len(content)} chars",
                    method="advanced",
                    duration=duration
                )
                
        except asyncio.TimeoutError:
            return ScrapingResult(
                success=False,
                error="Advanced scraper timeout (20s)",
                method="advanced", 
                duration=time.time() - start_time
            )
        except Exception as e:
            return ScrapingResult(
                success=False,
                error=str(e),
                method="advanced",
                duration=time.time() - start_time
            )
    
    async def _scrape_basic(self, url: str) -> ScrapingResult:
        """üì° Basic HTTP scraping con DEBUG DETTAGLIATO"""
        import aiohttp
        
        start_time = time.time()
        logger.info(f"üöÄ Basic HTTP starting for: {url}")
        
        try:
            # PROFESSIONAL UA ROTATION - Anti-WAF Defense
            headers = ua_rotator.get_complete_headers()
            logger.info(f"üé≠ Using rotated UA: {headers['User-Agent'][:50]}...")
            
            # EXPERT TIMEOUT CONFIGURATION - Granular control
            timeout = aiohttp.ClientTimeout(
                total=5.0,        # 5s total instead of 20s
                connect=2.0,      # 2s for connection establishment  
                sock_read=3.0     # 3s for reading response
            )
            logger.info(f"üîß Basic HTTP: Creating session with EXPERT timeouts (5s total)")
            
            # üÜï BROTLI FIX: Disable Brotli compression to avoid decode errors
            # Sites like fiat.it use Brotli (br) but aiohttp can't decode it
            headers_no_br = headers.copy()
            headers_no_br['Accept-Encoding'] = 'gzip, deflate'  # Remove 'br' support
            
            # First try with SSL verification, then fallback to SSL bypass
            # 3 attempts total to handle WAF challenges (Cloudflare, etc.)
            connectors = [
                aiohttp.TCPConnector(ssl=None),  # Normal SSL verification
                aiohttp.TCPConnector(ssl=False),  # SSL bypass for problematic sites
                aiohttp.TCPConnector(ssl=False)   # 3rd attempt for WAF challenge completion
            ]
            
            for i, connector in enumerate(connectors):
                try:
                    # üé≠ Human-like delay between attempts (WAF bypass)
                    if i > 0:
                        import random
                        delay = random.uniform(1.0, 2.5)
                        logger.info(f"‚è≥ Waiting {delay:.1f}s before retry (WAF challenge)...")
                        await asyncio.sleep(delay)
                    
                    logger.info(f"üåê Basic HTTP: Attempt {i+1}/3 - Making request to {url}")
                    async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                        # FOLLOW REDIRECTS - CRUCIAL for sites like mondo-convenienza.it!
                        # Use headers_no_br to avoid Brotli decode errors
                        async with session.get(url, headers=headers_no_br, allow_redirects=True, max_redirects=5) as response:
                            duration = time.time() - start_time
                            logger.info(f"üìä Basic HTTP: Got response status {response.status}")
                            
                            # ‚úÖ Accept all 2xx status codes (200-299), including 202 Accepted
                            if 200 <= response.status < 300:
                                content = await response.text()
                                content_length = len(content)
                                logger.info(f"‚úÖ Basic HTTP SUCCESS ({response.status}): {content_length} characters received")
                                return ScrapingResult(
                                    success=True,
                                    content=content,
                                    method="basic",
                                    duration=duration,
                                    content_length=content_length
                                )
                            else:
                                # üÜï MESSAGGI ERRORE CHIARI: Titoli descrittivi invece di solo codici
                                error_titles = {
                                    400: "Richiesta Non Valida",
                                    401: "Autenticazione Richiesta",
                                    403: "Accesso Negato - Sito Protetto da WAF/Firewall",
                                    404: "Pagina Non Trovata",
                                    429: "Troppe Richieste - Rate Limit",
                                    500: "Errore Server Interno",
                                    502: "Gateway Non Raggiungibile",
                                    503: "Servizio Temporaneamente Non Disponibile",
                                    504: "Timeout Gateway"
                                }
                                error_title = error_titles.get(response.status, "Errore HTTP")
                                error_msg = f"{error_title} (HTTP {response.status})"
                                logger.error(f"‚ùå Basic HTTP: {error_msg}")
                                
                                # üÜï PLAYWRIGHT FALLBACK per 403: DISABILITATO (Railway 512MB limit)
                                # Browser Pool non inizializzato in produzione ‚Üí skip fallback
                                if response.status == 403 and i == len(connectors) - 1:
                                    logger.warning(f"‚ö° Status 403 after 3 attempts - Site has aggressive WAF protection")
                                    # Skip Playwright fallback - would fail anyway with "not initialized"
                                    # Accept 403 as final failure to avoid false retry attempts
                                
                                if i == len(connectors) - 1:  # Last attempt (3/3)
                                    return ScrapingResult(
                                        success=False,
                                        error=error_msg,
                                        method="basic",
                                        duration=duration
                                    )
                                continue  # Try next connector
                except aiohttp.ClientConnectorSSLError as e:
                    logger.warning(f"üîí SSL Error on attempt {i+1}: {str(e)}")
                    if i == len(connectors) - 1:
                        raise e
                    continue  # Try next connector (SSL bypass)
                except aiohttp.ClientConnectorError as e:
                    logger.warning(f"üåê Connection Error on attempt {i+1}: {str(e)}")
                    if i == len(connectors) - 1:
                        raise e
                    continue
                except asyncio.TimeoutError as e:
                    logger.warning(f"‚è∞ Timeout on attempt {i+1}: {str(e)}")
                    if i == len(connectors) - 1:
                        raise e
                    continue
                except Exception as e:
                    error_msg = f"Attempt {i+1} failed: {type(e).__name__}: {str(e)}"
                    logger.warning(f"‚ö†Ô∏è {error_msg}")
                    if i == len(connectors) - 1:  # Last attempt failed
                        raise e
                    continue  # Try next connector
            
            # All HTTP connector methods failed - return failure
            return ScrapingResult(
                success=False,
                error="All HTTP connector methods failed",
                method="basic",
                duration=time.time() - start_time
            )
        
        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            logger.error(f"üí• Basic HTTP EXCEPTION: {error_type}: {error_msg}")
            logger.error(f"üîç Exception details: {repr(e)}")
            
            # Try to get more info about the exception
            if hasattr(e, 'args') and e.args:
                logger.error(f"üìù Exception args: {e.args}")
            
            # üåç CRITICAL FIX: Proxy fallback when HTTP methods fail with exceptions
            logger.info("üö® All HTTP methods failed with exceptions - trying PROXY FALLBACK")
            if proxy_system.should_use_proxy(url):
                try:
                    logger.info("üåç Attempting ScraperAPI fallback after HTTP failure")
                    headers = ua_rotator.get_complete_headers()
                    timeout = aiohttp.ClientTimeout(
                        total=120.0,     # 120s total for ScraperAPI - premium service needs time
                        connect=30.0,    # 30s to connect
                        sock_read=90.0   # 90s to read response
                    )
                    
                    proxy_content = await proxy_system.scrape_with_proxy(url, headers, timeout)
                    
                    if proxy_content and len(proxy_content) > 500:
                        logger.info(f"‚úÖ ScraperAPI SUCCESS after HTTP failure: {len(proxy_content)} chars")
                        return ScrapingResult(
                            success=True,
                            content=proxy_content,
                            method="scraperapi_fallback",
                            duration=time.time() - start_time,
                            content_length=len(proxy_content)
                        )
                    else:
                        logger.error("‚ùå ScraperAPI returned insufficient content")
                except Exception as proxy_e:
                    logger.error(f"üí• ScraperAPI fallback also failed: {proxy_e}")
            else:
                logger.info("üö´ Site not in proxy whitelist - skipping ScraperAPI")
            
            return ScrapingResult(
                success=False,
                error=f"{error_type}: {error_msg}" if error_msg else f"{error_type}: Unknown error",
                method="basic",
                duration=time.time() - start_time
            )
    
    async def _extract_keywords_smart(self, content: str, url: str, max_keywords: int) -> Dict[str, Any]:
        """üß† Estrazione keywords intelligente"""
        from bs4 import BeautifulSoup
        from .keyword_extraction import KeywordExtractor
        
        try:
            # Parse HTML
            soup = BeautifulSoup(content, 'html.parser')
            
            # Rimuovi elementi non necessari
            for element in soup(["script", "style", "meta", "link", "nav", "footer"]):
                element.decompose()
            
            # Estrai testo pulito
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            clean_text = ' '.join(chunk for chunk in chunks if chunk)
            
            # Metadata sito
            title = soup.find('title')
            title_text = title.get_text().strip() if title else ""
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', '').strip() if meta_desc else ""
            
            # Estrai keywords
            extractor = KeywordExtractor()
            keywords = extractor._process_text(clean_text)
            
            # Formato risultato
            return {
                'url': url,
                'keywords': [
                    {
                        'keyword': kw,
                        'frequency': max(1, 25 - i),
                        'relevance': 'high' if i < 5 else 'medium' if i < 12 else 'low',
                        'category': 'prodotto' if any(term in kw.lower() for term in ['mobili', 'arredamento', 'cucina', 'divano', 'letto']) else 'generale'
                    }
                    for i, kw in enumerate(keywords[:max_keywords])
                ],
                'total_keywords': len(keywords),
                'status': 'success',
                'title': title_text,
                'description': description,
                'content_length': len(clean_text),
                'scraping_method': 'hybrid_v2'
            }
            
        except Exception as e:
            logger.error(f"Keyword extraction failed: {e}")
            return {
                'url': url,
                'keywords': [],
                'status': 'extraction_failed',
                'error': str(e),
                'scraping_method': 'hybrid_v2'
            }
    
    def _update_stats(self, success_type: str, method: str, duration: float):
        """üìä Aggiorna statistiche performance"""
        self.stats[success_type] += 1
        
        # Aggiorna durata media
        current_avg = self.stats['average_duration']
        total_requests = self.stats['total_requests']
        self.stats['average_duration'] = (current_avg * (total_requests - 1) + duration) / total_requests
        
        # Distribuzione metodi
        if method not in self.stats['method_distribution']:
            self.stats['method_distribution'][method] = 0
        self.stats['method_distribution'][method] += 1
        
        # Success rate
        total_success = sum(self.stats[key] for key in ['browser_pool_success', 'scrapingbee_success', 'advanced_success', 'basic_success'])
        self.stats['success_rate'] = (total_success / total_requests) * 100 if total_requests > 0 else 0
    
    def _update_error_stats(self, error: str):
        """üìä Aggiorna statistiche errori"""
        error_type = 'unknown'
        
        if 'timeout' in error.lower():
            error_type = 'timeout'
        elif 'ssl' in error.lower() or 'certificate' in error.lower():
            error_type = 'ssl'
        elif 'cloudflare' in error.lower():
            error_type = 'cloudflare'
        elif 'connection' in error.lower():
            error_type = 'connection'
        elif 'forbidden' in error.lower() or '403' in error:
            error_type = 'blocked'
        
        if error_type not in self.stats['error_types']:
            self.stats['error_types'][error_type] = 0
        self.stats['error_types'][error_type] += 1
    
    async def get_enhanced_stats(self) -> Dict[str, Any]:
        """üìä Statistiche dettagliate sistema"""
        pool_stats = await browser_pool.get_pool_stats()
        
        return {
            'performance': self.stats,
            'browser_pool': pool_stats,
            'health_status': 'excellent' if self.stats['success_rate'] > 85 else 'good' if self.stats['success_rate'] > 70 else 'needs_attention',
            'recommendation': self._get_performance_recommendation()
        }
    
    def _get_performance_recommendation(self) -> str:
        """üí° Raccomandazioni performance"""
        success_rate = self.stats['success_rate']
        
        if success_rate > 95:
            return "üéâ Sistema perfetto! Performance eccellenti."
        elif success_rate > 85:
            return "‚úÖ Sistema stabile, considera ScrapingBee per scaling."
        elif success_rate > 70:
            return "‚ö†Ô∏è Performance buone, verifica proxy pool per miglioramenti."
        else:
            return "üö® Performance basse, necessario intervento immediato. Verifica configurazione."

# üåê Istanza globale V2
hybrid_scraper_v2 = HybridScraperV2()